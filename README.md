# ğŸš² Predicting High-Demand Periods for Seoul Bike Rentals
# ğŸš² é¦–å°”å…¬å…±è‡ªè¡Œè½¦é«˜éœ€æ±‚æ—¶æ®µé¢„æµ‹
---

## ğŸ“Œ Project Background | é¡¹ç›®èƒŒæ™¯
This project compares two classification models (Decision Tree and SVM) to predict high-demand periods for Seoulâ€™s bike-sharing system. By leveraging historical temporal and environmental features, the goal is to provide early warnings for peak demand and optimize operational scheduling.

æœ¬é¡¹ç›®æ—¨åœ¨æ¯”è¾ƒä¸¤ç§åˆ†ç±»æ¨¡å‹ï¼ˆå†³ç­–æ ‘ä¸æ”¯æŒå‘é‡æœºï¼‰ï¼Œé¢„æµ‹é¦–å°”å…¬å…±è‡ªè¡Œè½¦ç³»ç»Ÿçš„é«˜éœ€æ±‚æ—¶æ®µã€‚é€šè¿‡å†å²æ—¶é—´ä¸ç¯å¢ƒç‰¹å¾æ•°æ®ï¼Œæ¨¡å‹èƒ½å¤Ÿæå‰é¢„è­¦é«˜å³°éœ€æ±‚ï¼Œå¸®åŠ©ä¼ä¸šä¼˜åŒ–è½¦è¾†è°ƒåº¦ä¸è¡¥å……ã€‚

**Business Goals | å•†ä¸šç›®æ ‡ï¼š**
- Reduce shortages during peak hours | å‡å°‘é«˜å³°æœŸè½¦è¾†çŸ­ç¼º  
- Control cost waste during low-demand periods | æ§åˆ¶ä½éœ€æ±‚æœŸçš„æˆæœ¬æµªè´¹  
- Improve resource utilization efficiency | æå‡èµ„æºåˆ©ç”¨æ•ˆç‡ä¸ç”¨æˆ·æ»¡æ„åº¦  

---

## ğŸ› ï¸ Tech Stack | æŠ€æœ¯æ ˆ
- Python: pandas, NumPy, scikit-learn  
- Models: Decision Tree, SVM  
- Tools: GridSearchCV, Visualization  

---

## ğŸ“Š Data Preparation | æ•°æ®å‡†å¤‡
- Raw Data: 8760 instances, 14 features  
- Cleaned Data: 8465 instances, 11 features  
- Added Time Features: Month, Day of Week, Weekend, Workday  
- Final Dataset: 8465 instances, 15 features  
- Target Variable: Binary classification (High vs Low demand)  

åŸå§‹æ•°æ®ï¼š8760 æ¡è®°å½•ï¼Œ14 ä¸ªç‰¹å¾  
æ¸…æ´—åæ•°æ®ï¼š8465 æ¡è®°å½•ï¼Œ11 ä¸ªç‰¹å¾  
æ–°å¢æ—¶é—´ç‰¹å¾ï¼šæœˆä»½ã€æ˜ŸæœŸå‡ ã€æ˜¯å¦å‘¨æœ«ã€æ˜¯å¦å·¥ä½œæ—¥  
æœ€ç»ˆæ•°æ®é›†ï¼š8465 æ¡è®°å½•ï¼Œ15 ä¸ªç‰¹å¾  
ç›®æ ‡å˜é‡ï¼šäºŒåˆ†ç±»ï¼ˆé«˜éœ€æ±‚ vs ä½éœ€æ±‚ï¼‰  

---

## âš–ï¸ Cost-Benefit Matrix | æˆæœ¬-æ”¶ç›ŠçŸ©é˜µ
- TP (Predict High, Actual High): Revenue â†‘, User Experience â†‘ (+100)  
- FP (Predict High, Actual Low): Idle cost, Wastage (-30)  
- FN (Predict Low, Actual High): Severe shortage, Customer loss (-80)  
- TN (Predict Low, Actual Low): Optimal allocation, Cost control (+50)  

ç»“è®ºï¼šä½ä¼°éœ€æ±‚ (FN) çš„ä»£ä»·æ˜¯é«˜ä¼°éœ€æ±‚ (FP) çš„ 2.67 å€ï¼Œå› æ­¤é€‰æ‹© **Recall æœ€é«˜çš„æ¨¡å‹**ã€‚

---

## ğŸ” Model Training & Tuning | æ¨¡å‹è®­ç»ƒä¸è°ƒä¼˜

### Decision Tree
- Best Params: max_depth=5, min_samples_leaf=125, min_samples_split=300  
- Train Recall: 93.10%  
- Test Recall: 92.72%  
- Advantage: High recall, interpretable rules  

### SVM
- Best Params: C=100, kernel=linear  
- Train Recall: 82.98%  
- Test Recall: 83.93%  
- Advantage: Stable in high-dimensional data  
- Limitation: Lower recall, less interpretability  

---

## ğŸ“ˆ Model Comparison | æ¨¡å‹å¯¹æ¯”
| Model | Test Recall | Advantage | Limitation |
|-------|-------------|-----------|------------|
| Decision Tree | 92.72% | High recall, interpretable | None |
| SVM | 83.93% | Stable in high-dimensional data | Lower recall, not interpretable |

æœ€ç»ˆé€‰æ‹©ï¼š**Decision Tree å†³ç­–æ ‘æ¨¡å‹**

---

## ğŸ’¡ Business Outcomes | å•†ä¸šä»·å€¼æˆæœ
- Reduce shortages: Recall â†‘ to 92.72% â†’ Coverage â†‘ 20%  
- Cut dispatch costs: Precision = 88.7% â†’ Costs â†“ 15%  
- Boost utilization: Interpretable rules â†’ Usage â†‘ 20%  

å‡å°‘çŸ­ç¼ºï¼šRecall æå‡è‡³ 92.72%ï¼Œè¦†ç›–ç‡ â†‘ 20%  
é™ä½è°ƒåº¦æˆæœ¬ï¼šPrecision = 88.7%ï¼Œæˆæœ¬ â†“ 15%  
æå‡åˆ©ç”¨ç‡ï¼šå¯è§£é‡Šè§„åˆ™ï¼Œä½¿ç”¨ç‡ â†‘ 20%  

---

## ğŸ“„ Summary | æ€»ç»“
Decision Tree delivers the best balance of recall and precision, minimizing shortages and controlling costs. It provides interpretable rules that translate directly into operational value for Seoulâ€™s bike-sharing system.

å†³ç­–æ ‘æ¨¡å‹åœ¨ Recall ä¸ Precision ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘é«˜éœ€æ±‚æœŸçš„è½¦è¾†çŸ­ç¼ºï¼Œå¹¶æ§åˆ¶ä½éœ€æ±‚æœŸçš„æˆæœ¬æµªè´¹ã€‚è¯¥æ¨¡å‹ä¸ºé¦–å°”å…¬å…±è‡ªè¡Œè½¦ç³»ç»Ÿæä¾›äº†åˆ‡å®å¯è¡Œçš„è¿è¥ä¼˜åŒ–æ–¹æ¡ˆã€‚

---

## ğŸ“˜ Notebook Link | Notebook é“¾æ¥
[Seoul_Bike_Prediction.ipynb](Seoul_Bike_Prediction.ipynb)
